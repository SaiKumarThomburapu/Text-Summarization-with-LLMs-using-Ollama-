# Text Summarization with LLMs using Ollama 

This repository demonstrates the implementation of a text summarization system using Large Language Models (LLMs) through the Ollama server interface. It showcases how to extract concise summaries from long-form content using state-of-the-art open-source LLMs.

##  Objective

The goal of this notebook is to explore how LLMs can efficiently generate meaningful summaries from raw text using a minimal setup powered by the Ollama server.

##  Features

- Summarizes large text blocks into short, readable summaries.
- Uses local LLMs deployed via **Ollama** for inference.
- Simulates real-time summarization with a simple and interpretable Python workflow.
- Easily adaptable for chatbot integration and document summarization systems.

##  Tech Stack

- `Python`
- `Ollama` (LLM backend)
- `LLM model (e.g., LLaMA3 or other available models on Ollama)`
- `Notebook environment (Jupyter/Colab)`

##  How It Works

1. Input raw text (e.g., an article or paragraph).
2. The text is passed to the LLM via Ollama's API interface.
3. The model returns a concise summary of the input.
4. The summary is printed or returned as output.

 

